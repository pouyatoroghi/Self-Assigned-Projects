{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Method 1: Using a CNN in Pytorch"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-10-19T19:13:53.609875Z","iopub.status.busy":"2024-10-19T19:13:53.609190Z","iopub.status.idle":"2024-10-19T19:14:12.197627Z","shell.execute_reply":"2024-10-19T19:14:12.196459Z","shell.execute_reply.started":"2024-10-19T19:13:53.609824Z"},"trusted":true},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import librosa\n","import torch\n","import torchaudio\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tqdm import tqdm\n","import warnings\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","import torch.nn.functional as F\n","\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:14:12.200563Z","iopub.status.busy":"2024-10-19T19:14:12.199939Z","iopub.status.idle":"2024-10-19T19:24:54.092112Z","shell.execute_reply":"2024-10-19T19:24:54.089070Z","shell.execute_reply.started":"2024-10-19T19:14:12.200525Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Extracting features: 100%|██████████| 8732/8732 [10:41<00:00, 13.61it/s]\n"]}],"source":["dataset_path = \"/kaggle/input/urbansound8k\"\n","# Load the metadata CSV file containing labels and file information\n","metadata = pd.read_csv(os.path.join(dataset_path, 'UrbanSound8K.csv'))\n","\n","def extract_features(file_path):\n","    \"\"\"\n","    Extracts audio features from a given audio file.\n","\n","    Parameters:\n","        file_path (str): The path to the audio file.\n","\n","    Returns:\n","        np.ndarray: A feature vector containing aggregated audio features.\n","    \"\"\"\n","    # Load the audio file using librosa\n","    audio, sample_rate = librosa.load(file_path, sr=None)\n","\n","    # Compute the Mel Frequency Cepstral Coefficients (MFCCs)\n","    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n","\n","    # Calculate the first-order delta (rate of change) of the MFCCs\n","    mfcc_delta = librosa.feature.delta(mfccs, width=3)\n","    # Calculate the second-order delta (acceleration) of the MFCCs\n","    mfcc_delta2 = librosa.feature.delta(mfccs, order=2, width=3)\n","    \n","    # Aggregate features by calculating the mean and variance over time for each feature\n","    feature_vector = np.hstack([\n","        np.mean(mfccs, axis=1), np.var(mfccs, axis=1),      # Mean and variance of MFCCs\n","        np.mean(mfcc_delta, axis=1), np.var(mfcc_delta, axis=1),  # Mean and variance of first-order delta MFCCs\n","        np.mean(mfcc_delta2, axis=1), np.var(mfcc_delta2, axis=1) # Mean and variance of second-order delta MFCCs\n","    ])\n","    \n","    return feature_vector\n","\n","def prepare_data(metadata, dataset_path):\n","    \"\"\"\n","    Prepares the dataset by extracting features and labels from the audio files.\n","\n","    Parameters:\n","        metadata (DataFrame): The DataFrame containing file metadata.\n","        dataset_path (str): The base path to the dataset files.\n","\n","    Returns:\n","        tuple: A tuple containing an array of features and an array of labels.\n","    \"\"\"\n","    features = []  # List to store feature vectors\n","    labels = []    # List to store corresponding labels\n","    \n","    # Use tqdm to show progress while iterating through the metadata rows\n","    for i, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Extracting features\"):\n","        # Construct the full file path for the audio file\n","        file_path = os.path.join(dataset_path, f'fold{row[\"fold\"]}', row[\"slice_file_name\"])\n","        # Extract features from the audio file\n","        feature_vector = extract_features(file_path)\n","        features.append(feature_vector)  # Append the feature vector to the list\n","        labels.append(row[\"classID\"])     # Append the corresponding label to the list\n","    \n","    return np.array(features), np.array(labels)  # Return features and labels as numpy arrays\n","\n","# Prepare the dataset by extracting features and labels from the metadata\n","features, labels = prepare_data(metadata, dataset_path)"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:24:54.110286Z","iopub.status.busy":"2024-10-19T19:24:54.102845Z","iopub.status.idle":"2024-10-19T19:24:54.166653Z","shell.execute_reply":"2024-10-19T19:24:54.165422Z","shell.execute_reply.started":"2024-10-19T19:24:54.110159Z"},"trusted":true},"outputs":[],"source":["class UrbanSoundDataset(Dataset):\n","    \"\"\"\n","    Custom dataset class for the UrbanSound dataset.\n","    \n","    This class is used to wrap the features and labels into a format suitable for \n","    PyTorch's DataLoader, allowing for easy iteration and batching during training.\n","    \"\"\"\n","    \n","    def __init__(self, features, labels):\n","        \"\"\"\n","        Initializes the UrbanSoundDataset with features and labels.\n","        \n","        Parameters:\n","            features (np.ndarray): The array of extracted audio features.\n","            labels (np.ndarray): The array of corresponding labels for the features.\n","        \"\"\"\n","        self.features = features  # Store the extracted features\n","        self.labels = labels      # Store the corresponding labels\n","    \n","    def __len__(self):\n","        \"\"\"\n","        Returns the total number of samples in the dataset.\n","        \n","        Returns:\n","            int: The number of feature-label pairs in the dataset.\n","    \"\"\"\n","        return len(self.features)\n","    \n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Retrieves a sample from the dataset at the specified index.\n","        \n","        This method is called by the DataLoader to fetch individual samples.\n","        \n","        Parameters:\n","            idx (int): The index of the sample to retrieve.\n","        \n","        Returns:\n","            tuple: A tuple containing the feature tensor and the label tensor.\n","        \"\"\"\n","        return (\n","            torch.tensor(self.features[idx], dtype=torch.float32),  # Convert feature to tensor\n","            torch.tensor(self.labels[idx], dtype=torch.long)        # Convert label to tensor\n","        )\n","\n","# Split the dataset into training, testing, and validation sets\n","X_train, X_temp, y_train, y_temp = train_test_split(features, labels, test_size=0.3)  # 70% training, 30% temporary\n","X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5)        # Split temporary set into 15% test and 15% validation\n","\n","# Create dataset instances for each split\n","train_dataset = UrbanSoundDataset(X_train, y_train)  # Training dataset\n","test_dataset = UrbanSoundDataset(X_test, y_test)      # Testing dataset\n","val_dataset = UrbanSoundDataset(X_val, y_val)          # Validation dataset\n","\n","# Create data loaders for each dataset to facilitate batching and shuffling\n","train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)  # Training data loader with shuffling\n","test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # Testing data loader without shuffling\n","val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)      # Validation data loader without shuffling"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:24:54.170261Z","iopub.status.busy":"2024-10-19T19:24:54.169847Z","iopub.status.idle":"2024-10-19T19:24:54.242192Z","shell.execute_reply":"2024-10-19T19:24:54.241127Z","shell.execute_reply.started":"2024-10-19T19:24:54.170203Z"},"trusted":true},"outputs":[],"source":["class EnhancedSoundCNN(nn.Module):\n","    \"\"\"\n","    A Convolutional Neural Network (CNN) for sound classification.\n","\n","    This model employs 1D convolutional layers to learn features from audio data, \n","    followed by fully connected layers to classify the audio into predefined categories.\n","    \"\"\"\n","    \n","    def __init__(self):\n","        \"\"\"\n","        Initializes the EnhancedSoundCNN model by defining the architecture.\n","\n","        The model consists of several convolutional layers, pooling layers, \n","        dropout for regularization, and fully connected layers.\n","        \"\"\"\n","        super(EnhancedSoundCNN, self).__init__()\n","        \n","        # First convolutional layer: \n","        # Input channels = 1 (for mono audio), Output channels = 32, Kernel size = 3\n","        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n","        \n","        # Second convolutional layer:\n","        # Input channels = 32 (from previous layer), Output channels = 64\n","        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n","        \n","        # Third convolutional layer:\n","        # Input channels = 64 (from previous layer), Output channels = 128\n","        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)\n","        \n","        # Max pooling layer to reduce the dimensionality of the output from the convolutional layers\n","        self.pool = nn.MaxPool1d(2)  # Pooling size of 2\n","        \n","        # Fully connected layers:\n","        # The input size for the first fully connected layer is adjusted based on the output size from the convolutional layers\n","        self.fc1 = nn.Linear(128 * 30, 256)  # Example size, adjust based on the actual flattened output\n","        self.fc2 = nn.Linear(256, 128)        # Second fully connected layer\n","        self.fc3 = nn.Linear(128, 10)         # Output layer for 10 classes\n","        \n","        # Dropout layer to prevent overfitting\n","        self.dropout = nn.Dropout(0.3)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Defines the forward pass of the model.\n","\n","        The input tensor passes through the convolutional layers, followed by pooling, \n","        and then through the fully connected layers. \n","\n","        Parameters:\n","            x (torch.Tensor): Input tensor containing audio features.\n","\n","        Returns:\n","            torch.Tensor: Output tensor containing the class scores for the input audio.\n","        \"\"\"\n","        # Apply the first convolutional layer followed by ReLU activation and max pooling\n","        x = self.pool(F.relu(self.conv1(x)))\n","        \n","        # Apply the second convolutional layer followed by ReLU activation and max pooling\n","        x = self.pool(F.relu(self.conv2(x)))\n","        \n","        # Apply the third convolutional layer followed by ReLU activation and max pooling\n","        x = self.pool(F.relu(self.conv3(x)))\n","\n","        # Flatten the output for the fully connected layers\n","        x = x.view(x.size(0), -1)\n","\n","        # Pass through the first fully connected layer with ReLU activation\n","        x = F.relu(self.fc1(x))\n","        \n","        # Apply dropout for regularization\n","        x = self.dropout(x)\n","        \n","        # Pass through the second fully connected layer with ReLU activation\n","        x = F.relu(self.fc2(x))\n","        \n","        # Final output layer (class scores)\n","        x = self.fc3(x)\n","        return x\n","\n","# Determine the device to use (GPU if available, else CPU)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Instantiate the EnhancedSoundCNN model and move it to the appropriate device\n","model = EnhancedSoundCNN().to(device)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:24:54.243974Z","iopub.status.busy":"2024-10-19T19:24:54.243606Z","iopub.status.idle":"2024-10-19T19:32:01.054761Z","shell.execute_reply":"2024-10-19T19:32:01.053465Z","shell.execute_reply.started":"2024-10-19T19:24:54.243936Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Epoch 1/100: 100%|██████████| 48/48 [00:03<00:00, 12.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [1/100], Train Loss: 3.0609, Train Accuracy: 33.69%, Validation Accuracy: 53.13%\n","Model saved with validation accuracy: 53.13%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 2/100: 100%|██████████| 48/48 [00:03<00:00, 13.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [2/100], Train Loss: 1.1382, Train Accuracy: 61.49%, Validation Accuracy: 70.84%\n","Model saved with validation accuracy: 70.84%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 3/100: 100%|██████████| 48/48 [00:03<00:00, 13.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [3/100], Train Loss: 0.8421, Train Accuracy: 72.17%, Validation Accuracy: 76.26%\n","Model saved with validation accuracy: 76.26%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 4/100: 100%|██████████| 48/48 [00:04<00:00, 11.78it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [4/100], Train Loss: 0.6895, Train Accuracy: 77.36%, Validation Accuracy: 80.46%\n","Model saved with validation accuracy: 80.46%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 5/100: 100%|██████████| 48/48 [00:03<00:00, 13.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [5/100], Train Loss: 0.5513, Train Accuracy: 81.68%, Validation Accuracy: 82.52%\n","Model saved with validation accuracy: 82.52%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 6/100: 100%|██████████| 48/48 [00:03<00:00, 13.61it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [6/100], Train Loss: 0.4713, Train Accuracy: 85.23%, Validation Accuracy: 82.21%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 7/100: 100%|██████████| 48/48 [00:03<00:00, 13.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [7/100], Train Loss: 0.4108, Train Accuracy: 86.75%, Validation Accuracy: 85.80%\n","Model saved with validation accuracy: 85.80%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 8/100: 100%|██████████| 48/48 [00:03<00:00, 13.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [8/100], Train Loss: 0.3436, Train Accuracy: 88.84%, Validation Accuracy: 84.73%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 9/100: 100%|██████████| 48/48 [00:03<00:00, 13.63it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [9/100], Train Loss: 0.3057, Train Accuracy: 90.43%, Validation Accuracy: 86.56%\n","Model saved with validation accuracy: 86.56%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 10/100: 100%|██████████| 48/48 [00:03<00:00, 13.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [10/100], Train Loss: 0.2520, Train Accuracy: 91.97%, Validation Accuracy: 88.02%\n","Model saved with validation accuracy: 88.02%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 11/100: 100%|██████████| 48/48 [00:03<00:00, 13.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [11/100], Train Loss: 0.2557, Train Accuracy: 91.43%, Validation Accuracy: 87.25%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 12/100: 100%|██████████| 48/48 [00:03<00:00, 13.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [12/100], Train Loss: 0.2311, Train Accuracy: 92.29%, Validation Accuracy: 89.39%\n","Model saved with validation accuracy: 89.39%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 13/100: 100%|██████████| 48/48 [00:04<00:00, 11.14it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [13/100], Train Loss: 0.1836, Train Accuracy: 94.19%, Validation Accuracy: 89.16%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 14/100: 100%|██████████| 48/48 [00:03<00:00, 13.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [14/100], Train Loss: 0.1563, Train Accuracy: 95.17%, Validation Accuracy: 90.69%\n","Model saved with validation accuracy: 90.69%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 15/100: 100%|██████████| 48/48 [00:03<00:00, 12.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [15/100], Train Loss: 0.1402, Train Accuracy: 95.53%, Validation Accuracy: 89.24%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 16/100: 100%|██████████| 48/48 [00:04<00:00, 11.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [16/100], Train Loss: 0.1397, Train Accuracy: 95.44%, Validation Accuracy: 90.31%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 17/100: 100%|██████████| 48/48 [00:04<00:00, 11.96it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [17/100], Train Loss: 0.0961, Train Accuracy: 97.27%, Validation Accuracy: 91.91%\n","Model saved with validation accuracy: 91.91%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 18/100: 100%|██████████| 48/48 [00:04<00:00, 11.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [18/100], Train Loss: 0.1142, Train Accuracy: 95.98%, Validation Accuracy: 91.37%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 19/100: 100%|██████████| 48/48 [00:03<00:00, 12.34it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [19/100], Train Loss: 0.0891, Train Accuracy: 96.96%, Validation Accuracy: 90.00%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 20/100: 100%|██████████| 48/48 [00:03<00:00, 12.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [20/100], Train Loss: 0.0837, Train Accuracy: 97.41%, Validation Accuracy: 90.69%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 21/100: 100%|██████████| 48/48 [00:04<00:00, 10.75it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [21/100], Train Loss: 0.0830, Train Accuracy: 97.28%, Validation Accuracy: 92.37%\n","Model saved with validation accuracy: 92.37%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 22/100: 100%|██████████| 48/48 [00:03<00:00, 12.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [22/100], Train Loss: 0.0501, Train Accuracy: 98.54%, Validation Accuracy: 92.06%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 23/100: 100%|██████████| 48/48 [00:03<00:00, 12.11it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [23/100], Train Loss: 0.0415, Train Accuracy: 98.79%, Validation Accuracy: 90.99%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 24/100: 100%|██████████| 48/48 [00:03<00:00, 12.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [24/100], Train Loss: 0.0566, Train Accuracy: 98.46%, Validation Accuracy: 91.91%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 25/100: 100%|██████████| 48/48 [00:03<00:00, 12.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [25/100], Train Loss: 0.0830, Train Accuracy: 97.35%, Validation Accuracy: 91.45%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 26/100: 100%|██████████| 48/48 [00:03<00:00, 12.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [26/100], Train Loss: 0.0562, Train Accuracy: 98.07%, Validation Accuracy: 92.14%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 27/100: 100%|██████████| 48/48 [00:03<00:00, 12.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [27/100], Train Loss: 0.0462, Train Accuracy: 98.56%, Validation Accuracy: 92.44%\n","Model saved with validation accuracy: 92.44%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 28/100: 100%|██████████| 48/48 [00:04<00:00, 10.38it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [28/100], Train Loss: 0.0431, Train Accuracy: 98.61%, Validation Accuracy: 92.21%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 29/100: 100%|██████████| 48/48 [00:04<00:00, 11.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [29/100], Train Loss: 0.0484, Train Accuracy: 98.46%, Validation Accuracy: 91.60%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 30/100: 100%|██████████| 48/48 [00:03<00:00, 12.36it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [30/100], Train Loss: 0.1211, Train Accuracy: 95.98%, Validation Accuracy: 90.84%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 31/100: 100%|██████████| 48/48 [00:03<00:00, 12.31it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [31/100], Train Loss: 0.2141, Train Accuracy: 95.52%, Validation Accuracy: 80.76%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 32/100: 100%|██████████| 48/48 [00:03<00:00, 12.48it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [32/100], Train Loss: 0.2196, Train Accuracy: 93.52%, Validation Accuracy: 89.62%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 33/100: 100%|██████████| 48/48 [00:03<00:00, 12.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [33/100], Train Loss: 0.0618, Train Accuracy: 98.05%, Validation Accuracy: 90.84%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 34/100: 100%|██████████| 48/48 [00:03<00:00, 12.62it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [34/100], Train Loss: 0.0376, Train Accuracy: 98.97%, Validation Accuracy: 92.21%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 35/100: 100%|██████████| 48/48 [00:03<00:00, 12.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [35/100], Train Loss: 0.0275, Train Accuracy: 99.20%, Validation Accuracy: 92.37%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 36/100: 100%|██████████| 48/48 [00:04<00:00, 10.58it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [36/100], Train Loss: 0.0146, Train Accuracy: 99.56%, Validation Accuracy: 93.21%\n","Model saved with validation accuracy: 93.21%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 37/100: 100%|██████████| 48/48 [00:03<00:00, 12.26it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [37/100], Train Loss: 0.0127, Train Accuracy: 99.66%, Validation Accuracy: 93.51%\n","Model saved with validation accuracy: 93.51%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 38/100: 100%|██████████| 48/48 [00:03<00:00, 12.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [38/100], Train Loss: 0.0148, Train Accuracy: 99.57%, Validation Accuracy: 93.51%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 39/100: 100%|██████████| 48/48 [00:03<00:00, 12.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [39/100], Train Loss: 0.0131, Train Accuracy: 99.61%, Validation Accuracy: 93.82%\n","Model saved with validation accuracy: 93.82%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 40/100: 100%|██████████| 48/48 [00:03<00:00, 12.33it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [40/100], Train Loss: 0.0082, Train Accuracy: 99.72%, Validation Accuracy: 93.97%\n","Model saved with validation accuracy: 93.97%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 41/100: 100%|██████████| 48/48 [00:03<00:00, 12.21it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [41/100], Train Loss: 0.0106, Train Accuracy: 99.69%, Validation Accuracy: 93.44%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 42/100: 100%|██████████| 48/48 [00:03<00:00, 12.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [42/100], Train Loss: 0.0107, Train Accuracy: 99.66%, Validation Accuracy: 93.51%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 43/100: 100%|██████████| 48/48 [00:03<00:00, 12.24it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [43/100], Train Loss: 0.0121, Train Accuracy: 99.64%, Validation Accuracy: 93.89%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 44/100: 100%|██████████| 48/48 [00:04<00:00, 10.28it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [44/100], Train Loss: 0.0102, Train Accuracy: 99.75%, Validation Accuracy: 93.44%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 45/100: 100%|██████████| 48/48 [00:03<00:00, 12.16it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [45/100], Train Loss: 0.0097, Train Accuracy: 99.71%, Validation Accuracy: 93.36%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 46/100: 100%|██████████| 48/48 [00:04<00:00, 11.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [46/100], Train Loss: 0.0096, Train Accuracy: 99.72%, Validation Accuracy: 93.82%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 47/100: 100%|██████████| 48/48 [00:04<00:00, 11.90it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [47/100], Train Loss: 0.0113, Train Accuracy: 99.67%, Validation Accuracy: 92.29%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 48/100: 100%|██████████| 48/48 [00:04<00:00, 11.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [48/100], Train Loss: 0.0220, Train Accuracy: 99.31%, Validation Accuracy: 92.21%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 49/100: 100%|██████████| 48/48 [00:04<00:00, 11.95it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [49/100], Train Loss: 0.0980, Train Accuracy: 96.81%, Validation Accuracy: 90.61%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 50/100: 100%|██████████| 48/48 [00:03<00:00, 12.39it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [50/100], Train Loss: 0.0563, Train Accuracy: 98.05%, Validation Accuracy: 91.91%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 51/100: 100%|██████████| 48/48 [00:04<00:00, 11.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [51/100], Train Loss: 0.0456, Train Accuracy: 98.76%, Validation Accuracy: 92.29%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 52/100: 100%|██████████| 48/48 [00:04<00:00, 10.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [52/100], Train Loss: 0.0268, Train Accuracy: 99.05%, Validation Accuracy: 91.60%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 53/100: 100%|██████████| 48/48 [00:03<00:00, 12.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [53/100], Train Loss: 0.0419, Train Accuracy: 98.48%, Validation Accuracy: 91.68%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 54/100: 100%|██████████| 48/48 [00:04<00:00, 11.76it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [54/100], Train Loss: 0.0318, Train Accuracy: 98.90%, Validation Accuracy: 92.82%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 55/100: 100%|██████████| 48/48 [00:03<00:00, 12.23it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [55/100], Train Loss: 0.0173, Train Accuracy: 99.48%, Validation Accuracy: 92.52%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 56/100: 100%|██████████| 48/48 [00:03<00:00, 12.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [56/100], Train Loss: 0.0107, Train Accuracy: 99.62%, Validation Accuracy: 93.28%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 57/100: 100%|██████████| 48/48 [00:03<00:00, 12.30it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [57/100], Train Loss: 0.0088, Train Accuracy: 99.64%, Validation Accuracy: 93.74%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 58/100: 100%|██████████| 48/48 [00:04<00:00, 11.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [58/100], Train Loss: 0.0081, Train Accuracy: 99.69%, Validation Accuracy: 93.66%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 59/100: 100%|██████████| 48/48 [00:04<00:00, 10.10it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [59/100], Train Loss: 0.0084, Train Accuracy: 99.67%, Validation Accuracy: 94.27%\n","Model saved with validation accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 60/100: 100%|██████████| 48/48 [00:04<00:00, 11.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [60/100], Train Loss: 0.0053, Train Accuracy: 99.79%, Validation Accuracy: 93.97%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 61/100: 100%|██████████| 48/48 [00:04<00:00, 12.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [61/100], Train Loss: 0.0060, Train Accuracy: 99.79%, Validation Accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 62/100: 100%|██████████| 48/48 [00:04<00:00, 11.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [62/100], Train Loss: 0.0081, Train Accuracy: 99.79%, Validation Accuracy: 94.73%\n","Model saved with validation accuracy: 94.73%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 63/100: 100%|██████████| 48/48 [00:04<00:00, 11.87it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [63/100], Train Loss: 0.0053, Train Accuracy: 99.79%, Validation Accuracy: 94.50%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 64/100: 100%|██████████| 48/48 [00:04<00:00, 11.80it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [64/100], Train Loss: 0.0057, Train Accuracy: 99.77%, Validation Accuracy: 93.97%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 65/100: 100%|██████████| 48/48 [00:04<00:00, 11.55it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [65/100], Train Loss: 0.0060, Train Accuracy: 99.75%, Validation Accuracy: 94.43%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 66/100: 100%|██████████| 48/48 [00:04<00:00, 11.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [66/100], Train Loss: 0.0044, Train Accuracy: 99.79%, Validation Accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 67/100: 100%|██████████| 48/48 [00:04<00:00, 10.07it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [67/100], Train Loss: 0.0052, Train Accuracy: 99.77%, Validation Accuracy: 94.81%\n","Model saved with validation accuracy: 94.81%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 68/100: 100%|██████████| 48/48 [00:04<00:00, 11.72it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [68/100], Train Loss: 0.0049, Train Accuracy: 99.79%, Validation Accuracy: 94.73%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 69/100: 100%|██████████| 48/48 [00:03<00:00, 12.00it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [69/100], Train Loss: 0.0039, Train Accuracy: 99.84%, Validation Accuracy: 94.66%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 70/100: 100%|██████████| 48/48 [00:03<00:00, 12.20it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [70/100], Train Loss: 0.0044, Train Accuracy: 99.79%, Validation Accuracy: 94.66%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 71/100: 100%|██████████| 48/48 [00:04<00:00, 11.84it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [71/100], Train Loss: 0.0039, Train Accuracy: 99.79%, Validation Accuracy: 94.66%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 72/100: 100%|██████████| 48/48 [00:04<00:00, 11.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [72/100], Train Loss: 0.0044, Train Accuracy: 99.79%, Validation Accuracy: 95.11%\n","Model saved with validation accuracy: 95.11%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 73/100: 100%|██████████| 48/48 [00:04<00:00, 11.88it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [73/100], Train Loss: 0.0054, Train Accuracy: 99.79%, Validation Accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 74/100: 100%|██████████| 48/48 [00:04<00:00, 10.64it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [74/100], Train Loss: 0.0043, Train Accuracy: 99.85%, Validation Accuracy: 95.27%\n","Model saved with validation accuracy: 95.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 75/100: 100%|██████████| 48/48 [00:04<00:00, 11.85it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [75/100], Train Loss: 0.0039, Train Accuracy: 99.82%, Validation Accuracy: 94.66%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 76/100: 100%|██████████| 48/48 [00:04<00:00, 11.74it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [76/100], Train Loss: 0.0049, Train Accuracy: 99.80%, Validation Accuracy: 94.50%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 77/100: 100%|██████████| 48/48 [00:04<00:00, 11.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [77/100], Train Loss: 0.0052, Train Accuracy: 99.77%, Validation Accuracy: 93.97%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 78/100: 100%|██████████| 48/48 [00:04<00:00, 11.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [78/100], Train Loss: 0.0029, Train Accuracy: 99.82%, Validation Accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 79/100: 100%|██████████| 48/48 [00:04<00:00, 11.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [79/100], Train Loss: 0.0050, Train Accuracy: 99.75%, Validation Accuracy: 92.98%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 80/100: 100%|██████████| 48/48 [00:04<00:00, 11.77it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [80/100], Train Loss: 0.0974, Train Accuracy: 96.99%, Validation Accuracy: 88.17%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 81/100: 100%|██████████| 48/48 [00:03<00:00, 12.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [81/100], Train Loss: 0.2427, Train Accuracy: 92.51%, Validation Accuracy: 83.82%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 82/100: 100%|██████████| 48/48 [00:04<00:00, 10.70it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [82/100], Train Loss: 0.1663, Train Accuracy: 94.75%, Validation Accuracy: 90.84%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 83/100: 100%|██████████| 48/48 [00:04<00:00, 11.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [83/100], Train Loss: 0.0651, Train Accuracy: 97.79%, Validation Accuracy: 91.30%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 84/100: 100%|██████████| 48/48 [00:04<00:00, 11.37it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [84/100], Train Loss: 0.0328, Train Accuracy: 98.72%, Validation Accuracy: 91.91%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 85/100: 100%|██████████| 48/48 [00:04<00:00, 11.49it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [85/100], Train Loss: 0.0312, Train Accuracy: 99.10%, Validation Accuracy: 91.37%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 86/100: 100%|██████████| 48/48 [00:04<00:00, 11.15it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [86/100], Train Loss: 0.0281, Train Accuracy: 99.03%, Validation Accuracy: 92.98%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 87/100: 100%|██████████| 48/48 [00:04<00:00, 11.02it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [87/100], Train Loss: 0.0161, Train Accuracy: 99.46%, Validation Accuracy: 91.68%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 88/100: 100%|██████████| 48/48 [00:04<00:00, 10.98it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [88/100], Train Loss: 0.0147, Train Accuracy: 99.48%, Validation Accuracy: 93.66%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 89/100: 100%|██████████| 48/48 [00:04<00:00,  9.93it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [89/100], Train Loss: 0.0091, Train Accuracy: 99.67%, Validation Accuracy: 94.12%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 90/100: 100%|██████████| 48/48 [00:04<00:00, 10.83it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [90/100], Train Loss: 0.0045, Train Accuracy: 99.80%, Validation Accuracy: 93.74%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 91/100: 100%|██████████| 48/48 [00:04<00:00, 11.03it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [91/100], Train Loss: 0.0042, Train Accuracy: 99.84%, Validation Accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 92/100: 100%|██████████| 48/48 [00:04<00:00, 10.99it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [92/100], Train Loss: 0.0055, Train Accuracy: 99.74%, Validation Accuracy: 93.82%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 93/100: 100%|██████████| 48/48 [00:04<00:00, 10.91it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [93/100], Train Loss: 0.0048, Train Accuracy: 99.82%, Validation Accuracy: 94.12%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 94/100: 100%|██████████| 48/48 [00:04<00:00, 10.81it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [94/100], Train Loss: 0.0045, Train Accuracy: 99.82%, Validation Accuracy: 93.97%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 95/100: 100%|██████████| 48/48 [00:04<00:00, 10.71it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [95/100], Train Loss: 0.0041, Train Accuracy: 99.79%, Validation Accuracy: 93.97%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 96/100: 100%|██████████| 48/48 [00:04<00:00,  9.66it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [96/100], Train Loss: 0.0044, Train Accuracy: 99.79%, Validation Accuracy: 94.27%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 97/100: 100%|██████████| 48/48 [00:04<00:00, 10.47it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [97/100], Train Loss: 0.0036, Train Accuracy: 99.80%, Validation Accuracy: 94.05%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 98/100: 100%|██████████| 48/48 [00:04<00:00, 10.59it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [98/100], Train Loss: 0.0039, Train Accuracy: 99.84%, Validation Accuracy: 94.58%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 99/100: 100%|██████████| 48/48 [00:04<00:00, 10.46it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [99/100], Train Loss: 0.0041, Train Accuracy: 99.79%, Validation Accuracy: 93.82%\n"]},{"name":"stderr","output_type":"stream","text":["Training Epoch 100/100: 100%|██████████| 48/48 [00:04<00:00, 10.86it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [100/100], Train Loss: 0.0029, Train Accuracy: 99.84%, Validation Accuracy: 93.97%\n","Best Validation Accuracy: 95.27%\n","Best model loaded.\n"]}],"source":["def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n","    \"\"\"\n","    Trains the specified model using the provided training data and evaluation criteria.\n","\n","    Parameters:\n","        model (nn.Module): The neural network model to be trained.\n","        train_loader (DataLoader): DataLoader providing training data in batches.\n","        criterion (loss function): The loss function used to evaluate the model.\n","        optimizer (Optimizer): The optimization algorithm used to update model weights.\n","        num_epochs (int): The number of training epochs.\n","    \"\"\"\n","    model.train()  # Set the model to training mode\n","\n","    best_val_acc = 0  # Initialize the best validation accuracy tracker\n","    \n","    for epoch in range(num_epochs):\n","        running_loss = 0.0  # Initialize loss for the current epoch\n","        correct_train = 0    # Counter for correct predictions in training\n","        total_train = 0      # Total number of training samples\n","        \n","        # Iterate over batches in the training DataLoader with a progress bar\n","        for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n","            inputs = inputs.unsqueeze(1)  # Add a channel dimension for the model input\n","            optimizer.zero_grad()          # Zero the gradients from the previous iteration\n","            outputs = model(inputs)        # Forward pass: compute model outputs\n","            loss = criterion(outputs, labels)  # Calculate the loss using the criterion\n","            loss.backward()                # Backward pass: compute gradients\n","            optimizer.step()               # Update model parameters\n","            \n","            running_loss += loss.item()    # Accumulate the loss\n","\n","            # Calculate training accuracy\n","            _, predicted = torch.max(outputs, 1)  # Get the predicted class indices\n","            correct_train += (predicted == labels).sum().item()  # Count correct predictions\n","            total_train += labels.size(0)         # Update total training samples\n","\n","        # Compute average training loss and accuracy for this epoch\n","        train_accuracy = 100 * correct_train / total_train\n","        train_loss = running_loss / len(train_loader)\n","\n","        # Validation Phase\n","        model.eval()  # Set the model to evaluation mode\n","        correct_val = 0        \n","        total_val = 0\n","\n","        with torch.no_grad():  # Disable gradient calculation for validation\n","            for batch in val_loader:\n","                inputs, labels = batch\n","                inputs = inputs.unsqueeze(1)  # Add a channel dimension\n","                inputs, labels = inputs.to(device), labels.to(device)  # Move to the appropriate device\n","\n","                # Forward pass for validation\n","                outputs = model(inputs)\n","                loss = criterion(outputs, labels)  # Calculate validation loss\n","\n","                # Calculate validation accuracy\n","                _, predicted = torch.max(outputs, 1)  # Get the predicted class indices\n","                correct_val += (predicted == labels).sum().item()  # Count correct predictions\n","                total_val += labels.size(0)  # Update total validation samples\n","        \n","        # Compute validation accuracy for this epoch\n","        val_accuracy = 100 * correct_val / total_val\n","\n","        # Log the statistics for this epoch\n","        print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n","              f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, \"\n","              f\"Validation Accuracy: {val_accuracy:.2f}%\")\n","\n","        # Save the best model based on validation accuracy\n","        if val_accuracy > best_val_acc:\n","            best_val_acc = val_accuracy  # Update the best validation accuracy\n","            torch.save(model.state_dict(), \"best_model.pth\")  # Save the model parameters\n","            print(f\"Model saved with validation accuracy: {val_accuracy:.2f}%\")\n","\n","    print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")  # Log the best validation accuracy\n","\n","def load_best_model(model, save_path='best_model.pth'):\n","    \"\"\"\n","    Loads the model parameters from the best model file.\n","\n","    Parameters:\n","        model (nn.Module): The neural network model to load parameters into.\n","        save_path (str): The path to the saved model parameters file.\n","    \"\"\"\n","    model.load_state_dict(torch.load(save_path))  # Load the model parameters from file\n","    model.eval()  # Set the model to evaluation mode\n","    print(\"Best model loaded.\")  # Confirm loading of the best model\n","\n","# Train the model with specified parameters\n","train_model(model, train_loader, nn.CrossEntropyLoss(), optim.Adam(model.parameters(), lr=0.001), num_epochs=100)\n","\n","# Load the best model after training\n","load_best_model(model, 'best_model.pth')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:32:01.058493Z","iopub.status.busy":"2024-10-19T19:32:01.056684Z","iopub.status.idle":"2024-10-19T19:32:01.255028Z","shell.execute_reply":"2024-10-19T19:32:01.253773Z","shell.execute_reply.started":"2024-10-19T19:32:01.058408Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 92.90076335877863%\n"]}],"source":["def evaluate_model(model, test_loader):\n","    \"\"\"\n","    Evaluates the performance of the trained model on the test dataset.\n","\n","    Parameters:\n","        model (nn.Module): The neural network model to evaluate.\n","        test_loader (DataLoader): DataLoader providing test data in batches.\n","    \"\"\"\n","    model.eval()  # Set the model to evaluation mode to disable dropout and batch normalization\n","    correct = 0   # Initialize counter for correct predictions\n","    total = 0     # Initialize counter for total samples processed\n","\n","    # Disable gradient calculation for evaluation to save memory and computation\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.unsqueeze(1)  # Add a channel dimension to the input tensor\n","            outputs = model(inputs)        # Forward pass: compute model outputs\n","            \n","            # Get the predicted class indices with the highest scores\n","            _, predicted = torch.max(outputs, 1)\n","            total += labels.size(0)        # Update total number of samples processed\n","            correct += (predicted == labels).sum().item()  # Count correct predictions\n","\n","    # Calculate and print the accuracy of the model on the test set\n","    print(f'Accuracy: {100 * correct / total}%')\n","\n","# Evaluate the model using the test dataset\n","evaluate_model(model, test_loader)"]},{"cell_type":"markdown","metadata":{},"source":["### Overview\n","\n","#### Data Preparation\n","\n","1. **Label Encoding**:\n","   - Categorical labels representing different sound classes are transformed into numerical format through label encoding. This process ensures that the model can effectively interpret the class information, as many machine learning algorithms require numerical inputs.\n","\n","2. **Data Splitting**:\n","   - The dataset is divided into training, validation, and test sets using a stratified approach. This method maintains the class distribution across the splits. Typically, 70% of the data is used for training, with the remaining 30% equally divided into validation and test sets. This ensures that the model is evaluated on unseen data, which is crucial for assessing its generalization capabilities.\n","\n","#### Feature Extraction\n","\n","3. **Audio Feature Extraction**:\n","   - **Mel-Frequency Cepstral Coefficients (MFCCs)**: MFCCs are one of the most common features used in audio classification tasks. They provide a compact representation of the audio spectrum by mimicking the human ear's perception of sound. The extraction process typically involves:\n","     - **Framing**: The audio signal is divided into overlapping frames, usually around 20-40 ms in length.\n","     - **Windowing**: Each frame is multiplied by a window function (like the Hamming window) to minimize signal discontinuities at the edges.\n","     - **Fast Fourier Transform (FFT)**: The windowed frames are transformed into the frequency domain using FFT, which provides information on the frequency components of the audio signal.\n","     - **Mel Filter Bank**: The FFT results are passed through a set of triangular filters that are spaced according to the Mel scale, emphasizing frequencies that align with human auditory perception.\n","     - **DCT (Discrete Cosine Transform)**: The log-magnitude of the filtered frequencies is then transformed using DCT to produce the MFCCs, which are commonly used as input features for neural networks.\n","   - **Spectrograms**: Another effective method for feature extraction involves converting audio signals into spectrograms, which visualize the frequency content over time. Spectrograms are generated by:\n","     - Applying FFT to overlapping time windows of the audio signal.\n","     - Mapping the resulting frequency information into a 2D representation, where one axis represents time and the other represents frequency. The intensity of colors in the spectrogram indicates the amplitude of frequencies at different time intervals.\n","   - **Zero-Crossing Rate and Spectral Features**: Other audio features such as zero-crossing rate (the rate at which the signal changes sign) and spectral centroid (the \"center of mass\" of the spectrum) can also provide valuable information for classification tasks. These features help characterize the tonal qualities of the audio signals, enhancing the model's ability to distinguish between different sound classes.\n","\n","#### Model Architecture\n","\n","4. **Building the Model**:\n","   - The **Enhanced Sound CNN** is constructed using convolutional layers specifically designed for processing audio signals. The architecture includes three convolutional layers, each capable of learning hierarchical features from the input audio data. The first layer accepts a single-channel input, while subsequent layers extract progressively complex features, helping the model learn intricate patterns in the data.\n","\n","5. **Activation Functions**:\n","   - ReLU (Rectified Linear Unit) activation functions are employed in the hidden layers. ReLU introduces non-linearity into the model, which is crucial for learning complex relationships inherent in the audio data.\n","\n","6. **Regularization**:\n","   - Dropout layers are integrated between dense layers to combat overfitting. By randomly deactivating a fraction of neurons during training, the model becomes less dependent on specific features, enhancing its generalization ability on unseen data.\n","\n","#### Model Compilation\n","\n","7. **Compilation**:\n","   - The model is compiled using the **Adam optimizer**, which is known for its adaptive learning rate capabilities, allowing for efficient training. The categorical crossentropy loss function is utilized for multi-class classification tasks, quantifying the discrepancy between predicted and actual class distributions.\n","\n","#### Model Training\n","\n","8. **Training the Model**:\n","   - The model undergoes training over multiple epochs, processing the training dataset and updating its weights based on computed losses. The validation dataset is used to monitor performance and make adjustments to prevent overfitting, helping the model learn to classify audio signals accurately.\n","\n","#### Model Evaluation\n","\n","9. **Evaluation**:\n","   - After training, the model's performance is assessed on the test dataset, yielding key metrics such as accuracy and loss. These metrics are vital for understanding how well the model generalizes to new, unseen data, which is critical for its application in real-world scenarios.\n","\n","### Summary\n","This comprehensive methodology integrates data preparation, feature extraction techniques, model design, and training protocols to ensure robust sound classification. Each step is essential for building a model that effectively generalizes and classifies sound events in the UrbanSound8K dataset. By leveraging established practices in deep learning and audio processing, the model is well-equipped to tackle challenges in audio classification tasks."]},{"cell_type":"markdown","metadata":{},"source":["---"]},{"cell_type":"markdown","metadata":{},"source":["# Method 2: Using a MLP in Tensorflow"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:32:01.257876Z","iopub.status.busy":"2024-10-19T19:32:01.256892Z","iopub.status.idle":"2024-10-19T19:32:01.273284Z","shell.execute_reply":"2024-10-19T19:32:01.271553Z","shell.execute_reply.started":"2024-10-19T19:32:01.257818Z"},"trusted":true},"outputs":[],"source":["# Encode the labels into a one-hot format for multi-class classification\n","le = LabelEncoder()  # Initialize the label encoder to convert class labels to numerical format\n","yy = to_categorical(le.fit_transform(labels))  # Transform labels into categorical (one-hot encoded) format\n","\n","# Split the features and encoded labels into training and temporary sets (70% training, 30% for validation/testing)\n","X_train, X_temp, y_train, y_temp = train_test_split(features, yy, test_size=0.3)\n","\n","# Further split the temporary set into testing and validation sets (50% testing, 50% validation of the temporary set)\n","X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:32:01.275478Z","iopub.status.busy":"2024-10-19T19:32:01.275045Z","iopub.status.idle":"2024-10-19T19:32:01.517706Z","shell.execute_reply":"2024-10-19T19:32:01.516526Z","shell.execute_reply.started":"2024-10-19T19:32:01.275435Z"},"trusted":true},"outputs":[],"source":["# Build the Multi-Layer Perceptron (MLP) model for sound classification\n","model = Sequential()  # Initialize the sequential model to stack layers\n","\n","# Add the input layer and first hidden layer with 512 neurons\n","# The input shape is set to (240,) to match the feature vector size\n","model.add(Dense(512, input_shape=(240,), activation='relu'))  \n","\n","# Apply dropout regularization to reduce overfitting by randomly setting 30% of the input units to 0 during training\n","model.add(Dropout(0.3))  \n","\n","# Add the second hidden layer with 256 neurons\n","model.add(Dense(256, activation='relu'))  \n","\n","# Apply dropout to the second hidden layer\n","model.add(Dropout(0.3))  \n","\n","# Add the third hidden layer with 128 neurons\n","model.add(Dense(128, activation='relu'))  \n","\n","# Apply dropout to the third hidden layer\n","model.add(Dropout(0.3))  \n","\n","# Add the fourth hidden layer with 64 neurons\n","model.add(Dense(64, activation='relu'))  \n","\n","# Apply dropout to the fourth hidden layer\n","model.add(Dropout(0.3))  \n","\n","# Add the output layer with 10 neurons for the 10 classes in the UrbanSound8K dataset\n","# The softmax activation function is used to output probabilities for each class\n","model.add(Dense(10, activation='softmax'))  "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-10-19T19:32:01.520457Z","iopub.status.busy":"2024-10-19T19:32:01.519914Z","iopub.status.idle":"2024-10-19T19:33:01.556226Z","shell.execute_reply":"2024-10-19T19:33:01.554626Z","shell.execute_reply.started":"2024-10-19T19:32:01.520409Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - accuracy: 0.1454 - loss: 96.0347 - val_accuracy: 0.3359 - val_loss: 3.5937\n","Epoch 2/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.1741 - loss: 13.7648 - val_accuracy: 0.2290 - val_loss: 2.2126\n","Epoch 3/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.1881 - loss: 7.2390 - val_accuracy: 0.2389 - val_loss: 2.3076\n","Epoch 4/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.1825 - loss: 4.2332 - val_accuracy: 0.2840 - val_loss: 2.0631\n","Epoch 5/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2224 - loss: 3.0310 - val_accuracy: 0.2939 - val_loss: 2.0455\n","Epoch 6/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.2341 - loss: 2.5069 - val_accuracy: 0.2847 - val_loss: 2.0337\n","Epoch 7/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.2350 - loss: 2.2791 - val_accuracy: 0.2939 - val_loss: 2.0330\n","Epoch 8/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.2282 - loss: 2.1436 - val_accuracy: 0.3084 - val_loss: 1.9620\n","Epoch 9/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2476 - loss: 2.1257 - val_accuracy: 0.3183 - val_loss: 1.9118\n","Epoch 10/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2576 - loss: 2.1125 - val_accuracy: 0.3221 - val_loss: 1.8632\n","Epoch 11/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2837 - loss: 2.0113 - val_accuracy: 0.3389 - val_loss: 1.8549\n","Epoch 12/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2971 - loss: 2.0127 - val_accuracy: 0.3260 - val_loss: 1.8510\n","Epoch 13/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.2949 - loss: 1.9692 - val_accuracy: 0.3435 - val_loss: 1.8118\n","Epoch 14/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.3101 - loss: 1.9749 - val_accuracy: 0.3489 - val_loss: 1.7542\n","Epoch 15/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3101 - loss: 1.9039 - val_accuracy: 0.3634 - val_loss: 1.7328\n","Epoch 16/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3158 - loss: 1.8735 - val_accuracy: 0.3771 - val_loss: 1.6914\n","Epoch 17/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3188 - loss: 1.8414 - val_accuracy: 0.3740 - val_loss: 1.6876\n","Epoch 18/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3302 - loss: 1.8104 - val_accuracy: 0.4198 - val_loss: 1.6002\n","Epoch 19/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3665 - loss: 1.7596 - val_accuracy: 0.4023 - val_loss: 1.5962\n","Epoch 20/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3933 - loss: 1.7007 - val_accuracy: 0.4427 - val_loss: 1.5623\n","Epoch 21/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.3830 - loss: 1.7159 - val_accuracy: 0.4725 - val_loss: 1.5116\n","Epoch 22/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.3937 - loss: 1.7646 - val_accuracy: 0.4687 - val_loss: 1.5138\n","Epoch 23/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4304 - loss: 1.6208 - val_accuracy: 0.4840 - val_loss: 1.5016\n","Epoch 24/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.4226 - loss: 1.6656 - val_accuracy: 0.4985 - val_loss: 1.4640\n","Epoch 25/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4526 - loss: 1.5548 - val_accuracy: 0.5076 - val_loss: 1.4275\n","Epoch 26/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4530 - loss: 1.5596 - val_accuracy: 0.5374 - val_loss: 1.3661\n","Epoch 27/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4633 - loss: 1.5641 - val_accuracy: 0.5489 - val_loss: 1.3814\n","Epoch 28/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4587 - loss: 1.5102 - val_accuracy: 0.5634 - val_loss: 1.3491\n","Epoch 29/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4792 - loss: 1.5033 - val_accuracy: 0.5656 - val_loss: 1.3311\n","Epoch 30/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4915 - loss: 1.4774 - val_accuracy: 0.5794 - val_loss: 1.2867\n","Epoch 31/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5087 - loss: 1.4340 - val_accuracy: 0.6038 - val_loss: 1.2343\n","Epoch 32/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5155 - loss: 1.4166 - val_accuracy: 0.6015 - val_loss: 1.2505\n","Epoch 33/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5220 - loss: 1.4049 - val_accuracy: 0.6168 - val_loss: 1.2173\n","Epoch 34/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5312 - loss: 1.3762 - val_accuracy: 0.6115 - val_loss: 1.1973\n","Epoch 35/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5292 - loss: 1.3813 - val_accuracy: 0.6229 - val_loss: 1.2188\n","Epoch 36/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5513 - loss: 1.3268 - val_accuracy: 0.6427 - val_loss: 1.1751\n","Epoch 37/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5629 - loss: 1.3000 - val_accuracy: 0.6336 - val_loss: 1.1831\n","Epoch 38/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5604 - loss: 1.3057 - val_accuracy: 0.6397 - val_loss: 1.1285\n","Epoch 39/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5834 - loss: 1.2293 - val_accuracy: 0.6603 - val_loss: 1.1011\n","Epoch 40/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5773 - loss: 1.2484 - val_accuracy: 0.6595 - val_loss: 1.0676\n","Epoch 41/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5886 - loss: 1.2181 - val_accuracy: 0.6672 - val_loss: 1.0843\n","Epoch 42/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5949 - loss: 1.1784 - val_accuracy: 0.6710 - val_loss: 1.0502\n","Epoch 43/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6028 - loss: 1.1749 - val_accuracy: 0.6863 - val_loss: 1.0115\n","Epoch 44/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6027 - loss: 1.1902 - val_accuracy: 0.6832 - val_loss: 1.0329\n","Epoch 45/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6129 - loss: 1.1717 - val_accuracy: 0.6817 - val_loss: 1.0091\n","Epoch 46/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6212 - loss: 1.1435 - val_accuracy: 0.6855 - val_loss: 0.9945\n","Epoch 47/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6115 - loss: 1.1347 - val_accuracy: 0.6824 - val_loss: 0.9742\n","Epoch 48/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6260 - loss: 1.1215 - val_accuracy: 0.6664 - val_loss: 1.0361\n","Epoch 49/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6246 - loss: 1.1386 - val_accuracy: 0.7061 - val_loss: 0.9406\n","Epoch 50/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6308 - loss: 1.1054 - val_accuracy: 0.7015 - val_loss: 0.9229\n","Epoch 51/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6621 - loss: 1.0622 - val_accuracy: 0.7229 - val_loss: 0.8905\n","Epoch 52/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6654 - loss: 1.0034 - val_accuracy: 0.7053 - val_loss: 0.9121\n","Epoch 53/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6472 - loss: 1.0483 - val_accuracy: 0.7351 - val_loss: 0.8627\n","Epoch 54/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6785 - loss: 0.9979 - val_accuracy: 0.7298 - val_loss: 0.8406\n","Epoch 55/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6798 - loss: 0.9875 - val_accuracy: 0.7466 - val_loss: 0.8284\n","Epoch 56/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6931 - loss: 0.9532 - val_accuracy: 0.7382 - val_loss: 0.8210\n","Epoch 57/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6910 - loss: 0.9271 - val_accuracy: 0.7397 - val_loss: 0.8017\n","Epoch 58/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7023 - loss: 0.9138 - val_accuracy: 0.7298 - val_loss: 0.8313\n","Epoch 59/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7127 - loss: 0.8641 - val_accuracy: 0.7489 - val_loss: 0.8249\n","Epoch 60/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6974 - loss: 0.9160 - val_accuracy: 0.7313 - val_loss: 0.8617\n","Epoch 61/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7046 - loss: 0.9088 - val_accuracy: 0.7443 - val_loss: 0.7983\n","Epoch 62/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7056 - loss: 0.8892 - val_accuracy: 0.7573 - val_loss: 0.7737\n","Epoch 63/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7202 - loss: 0.8863 - val_accuracy: 0.7443 - val_loss: 0.7940\n","Epoch 64/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7102 - loss: 0.8636 - val_accuracy: 0.7542 - val_loss: 0.7499\n","Epoch 65/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.7341 - loss: 0.8259 - val_accuracy: 0.7656 - val_loss: 0.7202\n","Epoch 66/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7395 - loss: 0.7999 - val_accuracy: 0.7763 - val_loss: 0.7678\n","Epoch 67/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7310 - loss: 0.8281 - val_accuracy: 0.7634 - val_loss: 0.7380\n","Epoch 68/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7295 - loss: 0.8301 - val_accuracy: 0.7557 - val_loss: 0.7600\n","Epoch 69/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7371 - loss: 0.8143 - val_accuracy: 0.7740 - val_loss: 0.7159\n","Epoch 70/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7686 - loss: 0.7248 - val_accuracy: 0.7649 - val_loss: 0.7193\n","Epoch 71/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7513 - loss: 0.7885 - val_accuracy: 0.7740 - val_loss: 0.7049\n","Epoch 72/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7458 - loss: 0.7597 - val_accuracy: 0.7702 - val_loss: 0.7105\n","Epoch 73/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7527 - loss: 0.7532 - val_accuracy: 0.7771 - val_loss: 0.6970\n","Epoch 74/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7510 - loss: 0.7715 - val_accuracy: 0.7740 - val_loss: 0.6840\n","Epoch 75/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7606 - loss: 0.7373 - val_accuracy: 0.7756 - val_loss: 0.7027\n","Epoch 76/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7739 - loss: 0.6976 - val_accuracy: 0.7718 - val_loss: 0.6878\n","Epoch 77/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7723 - loss: 0.6883 - val_accuracy: 0.7901 - val_loss: 0.6604\n","Epoch 78/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7750 - loss: 0.6852 - val_accuracy: 0.7809 - val_loss: 0.6872\n","Epoch 79/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7713 - loss: 0.7008 - val_accuracy: 0.7855 - val_loss: 0.6741\n","Epoch 80/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7789 - loss: 0.7012 - val_accuracy: 0.7710 - val_loss: 0.6981\n","Epoch 81/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7736 - loss: 0.7174 - val_accuracy: 0.7779 - val_loss: 0.6635\n","Epoch 82/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7780 - loss: 0.6630 - val_accuracy: 0.7901 - val_loss: 0.6535\n","Epoch 83/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7838 - loss: 0.6649 - val_accuracy: 0.7931 - val_loss: 0.6515\n","Epoch 84/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7796 - loss: 0.6686 - val_accuracy: 0.8084 - val_loss: 0.6235\n","Epoch 85/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7859 - loss: 0.6487 - val_accuracy: 0.7985 - val_loss: 0.6555\n","Epoch 86/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7730 - loss: 0.7046 - val_accuracy: 0.7885 - val_loss: 0.6887\n","Epoch 87/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.7685 - loss: 0.7173 - val_accuracy: 0.7954 - val_loss: 0.6334\n","Epoch 88/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7805 - loss: 0.6782 - val_accuracy: 0.7924 - val_loss: 0.6407\n","Epoch 89/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7933 - loss: 0.6464 - val_accuracy: 0.8015 - val_loss: 0.6009\n","Epoch 90/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.7933 - loss: 0.6301 - val_accuracy: 0.7985 - val_loss: 0.6369\n","Epoch 91/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7986 - loss: 0.6207 - val_accuracy: 0.8069 - val_loss: 0.5972\n","Epoch 92/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8034 - loss: 0.6180 - val_accuracy: 0.8145 - val_loss: 0.5876\n","Epoch 93/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8200 - loss: 0.5598 - val_accuracy: 0.8237 - val_loss: 0.5524\n","Epoch 94/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8067 - loss: 0.6033 - val_accuracy: 0.8115 - val_loss: 0.5685\n","Epoch 95/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8181 - loss: 0.5827 - val_accuracy: 0.7977 - val_loss: 0.6410\n","Epoch 96/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8038 - loss: 0.6246 - val_accuracy: 0.8168 - val_loss: 0.5862\n","Epoch 97/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8092 - loss: 0.5989 - val_accuracy: 0.8076 - val_loss: 0.5649\n","Epoch 98/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8160 - loss: 0.5580 - val_accuracy: 0.8023 - val_loss: 0.5900\n","Epoch 99/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8169 - loss: 0.5882 - val_accuracy: 0.8206 - val_loss: 0.5709\n","Epoch 100/100\n","\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.8201 - loss: 0.5668 - val_accuracy: 0.8069 - val_loss: 0.5889\n","\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8257 - loss: 0.6652\n","Test Accuracy: 0.8198473453521729\n"]}],"source":["# Compile the model with the specified optimizer, loss function, and metrics for evaluation\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model on the training dataset while validating on the validation set\n","history = model.fit(X_train, y_train, epochs=100, batch_size=128, validation_data=(X_val, y_val), verbose=1)\n","\n","# Evaluate the trained model's performance on the test dataset\n","test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n","\n","# Print the test accuracy to see how well the model generalizes to new data\n","print(f'Test Accuracy: {test_acc}')"]},{"cell_type":"markdown","metadata":{},"source":["### Overview\n","\n","#### Data Preparation\n","\n","1. **Label Encoding and One-Hot Encoding**:\n","   - Categorical labels representing different sound classes are first transformed into numerical format using label encoding. This step is essential for algorithms that require numerical inputs.\n","   - Following this, one-hot encoding converts these integer labels into a binary format suitable for multi-class classification. This ensures that each class is represented distinctly, allowing the model to treat each class independently during training.\n","\n","2. **Data Splitting**:\n","   - The dataset is split into training, validation, and test sets using a stratified approach. Initially, the dataset is divided so that 70% is allocated for training, while the remaining 30% is set aside for further evaluation.\n","   - This temporary set is subsequently split equally into validation and test sets, ensuring that the model is evaluated on unseen data at each stage.\n","\n","#### Model Architecture\n","\n","3. **Building the Model**:\n","   - The Multi-Layer Perceptron (MLP) model is constructed using a sequential architecture, where layers are stacked linearly. This structure is beneficial for capturing complex relationships in data.\n","   - Each dense layer in the network connects all neurons from the previous layer to the current one. The model begins with a substantial number of neurons (512 in the first hidden layer) and progressively decreases the number of neurons in subsequent layers. This approach facilitates the extraction of hierarchical features.\n","\n","4. **Activation Functions**:\n","   - The ReLU (Rectified Linear Unit) activation function is utilized in the hidden layers, introducing non-linearity to the model. This capability is crucial for learning complex patterns inherent in the audio data.\n","\n","5. **Regularization**:\n","   - Dropout layers are integrated between dense layers to mitigate overfitting. By randomly setting a fraction of the inputs to zero during training, the model becomes less reliant on specific neurons, enhancing its generalization capabilities.\n","\n","#### Model Compilation\n","\n","6. **Compilation**:\n","   - The model is compiled with the Adam optimizer, which is known for its efficiency in adjusting learning rates based on the training dynamics. This optimizer helps achieve faster convergence compared to traditional stochastic gradient descent methods.\n","   - The loss function employed is categorical crossentropy, ideal for multi-class classification tasks. It quantifies the difference between the predicted probability distribution and the actual distribution of classes, guiding the optimization process effectively.\n","\n","#### Model Training\n","\n","7. **Training the Model**:\n","   - The model undergoes training using the `fit` method, where it processes the training dataset over multiple epochs. During training, the model updates its weights based on the computed loss, gradually learning to classify sounds accurately.\n","   - The validation dataset is also used during training to monitor performance, allowing for adjustments to be made to avoid overfitting.\n","\n","#### Model Evaluation\n","\n","8. **Evaluation**:\n","   - After training, the model’s performance is assessed on the test dataset using the `evaluate` method. This step provides crucial metrics, such as loss and accuracy, which indicate how well the model generalizes to unseen data.\n","   - Finally, the test accuracy is printed, giving an immediate understanding of the model's effectiveness in classifying sound events from the UrbanSound8K dataset.\n","\n","### Summary\n","This systematic approach combines data preparation, model architecture design, regularization techniques, and rigorous training and evaluation methods. Each phase is crucial for ensuring that the final model is robust, generalizes well, and effectively classifies sound events within the UrbanSound8K dataset."]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":500970,"sourceId":928025,"sourceType":"datasetVersion"}],"dockerImageVersionId":30786,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":4}
